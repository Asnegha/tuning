| distributed init (rank 0): env://, gpu 0
[12:32:52.099295] job dir: /raid/speech/snegha/prompt_tuning/llama_adapter/LLaMA-Adapter/alpaca_finetuning_v1
[12:32:52.099472] Namespace(batch_size=16,
epochs=5,
accum_iter=1,
llama_model_path='model/LLaMA-7B/',
model='Llama7B_adapter',
adapter_layer=30,
adapter_len=10,
max_seq_len=512,
weight_decay=0.02,
lr=None,
blr=0.009,
min_lr=0.0,
warmup_epochs=2,
data_path='train_data_hindi.json',
output_dir='./checkpoint2/',
log_dir='./output_dir',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=0,
gpu=0,
distributed=True,
dist_backend='nccl')
[12:32:52.144310] <__main__.InstructionDataset object at 0x712a4b6d6880>
[12:32:52.144399] <__main__.InstructionDataset object at 0x712a417bb460>
[12:32:52.144505] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x712b1d195280>
[12:33:00.688597] model/LLaMA-7B//consolidated.00.pth
[12:33:03.819861] before tensor([ 0.9595, -1.0186, -0.8481,  0.8662,  0.5752,  0.8911,  2.1309, -0.2020,
         0.0273, -0.1974, -1.8271,  0.8550,  1.4365, -2.4609, -1.0205, -0.3181,
         0.0668, -1.0205, -0.3958, -0.5156, -0.9902,  0.7544, -1.0127, -0.2532,
         1.4873, -0.5283, -0.1315, -1.5303, -0.4106, -0.2196, -0.4819,  0.8408,
        -0.1748,  0.7041, -0.6504,  0.1998, -0.4763,  0.8594, -0.7812, -1.8701,
         1.2051,  0.1137,  1.1875, -2.3262, -0.0590, -0.0160,  0.3894,  1.6719,
        -1.6943,  0.6851,  0.9214, -0.1519, -0.1047, -0.2440, -0.0649,  1.1670,
        -0.3687,  0.7622,  0.8174,  1.1162, -1.3203, -0.9849, -0.5518,  0.3350,
        -1.4873, -2.7090,  0.4480,  1.0127, -0.6089,  0.5459,  1.0967,  1.7197,
        -1.4541,  1.0000,  0.6074,  2.3008,  1.1826,  0.5410,  1.2617,  0.6890,
         0.1127, -0.0465,  0.4167, -1.4033,  1.4189,  0.9946,  0.1852,  0.1962,
        -0.6353,  1.6953, -0.3777, -0.1997,  0.1530,  1.5889, -1.3457,  1.1514,
         0.1807, -1.1338, -0.1315, -0.2639,  0.3262,  0.3628, -0.4165,  0.2747,
        -2.4219,  0.6094,  0.7480,  0.7866, -0.4482, -0.4260, -0.1071,  1.3291,
         1.0977, -0.7358, -1.1484, -1.6699, -0.8960,  0.7285,  0.3044,  0.7134,
        -0.3215,  0.6455,  0.1552, -1.6084,  0.2117, -1.1299, -0.7910,  1.8027,
         1.0771,  2.2539,  0.0499, -1.0410, -0.0100, -0.9282,  0.6572, -0.1356,
        -0.1686, -0.8350, -0.5762, -0.4536, -0.8706,  0.1323,  0.2004,  2.5410,
         0.1331, -0.0954,  0.3496, -0.3103, -0.5703, -2.2383,  0.5767,  0.0640,
         0.4197,  0.3945, -1.1592,  0.7593, -0.7637, -0.5913,  1.3115,  0.7900,
        -1.6758,  0.2888, -0.3428,  2.7500,  0.3540, -0.8267,  1.0527,  0.0408,
        -0.1361, -0.7979,  1.3145,  1.1074,  1.1279, -1.0986,  1.7354, -0.7271,
         0.3013,  0.1631, -0.5503, -0.2791,  0.9600,  0.2188,  0.3813, -0.3435,
        -1.3955,  0.2903, -0.0842, -1.1631,  0.5801,  0.1855, -0.1356, -0.8862,
         0.2664,  0.4216, -1.5156,  0.3689, -0.9087, -1.2354, -0.3438,  0.9087,
        -0.2300,  0.8818,  0.7207, -1.4102, -0.4443,  0.4087,  1.2598, -0.3354,
        -0.3625,  0.6211,  0.8740, -1.6094,  2.1836, -0.4285,  0.2947, -0.4971,
        -0.2617,  0.5591, -1.1387,  0.4563,  1.2881,  1.5928,  1.2373, -0.0130,
        -0.2220,  0.4207,  1.5371,  0.2457, -1.5879,  0.3501,  0.4390,  0.7451,
         0.2499, -0.4124, -1.4043, -1.2852, -0.4409, -0.0537,  0.5933,  0.1447,
         0.0162,  0.3679,  0.8945, -1.3701, -0.4172,  0.4641,  2.7520,  0.9370,
         0.6445, -0.6440, -0.0337,  1.0742, -0.0538, -0.5278, -1.7207,  0.5713,
         1.8037, -0.4568, -0.1954,  0.5186,  1.5752, -0.8052, -0.6436, -0.3748,
        -1.0117,  0.3923,  0.6963, -0.1213,  0.1094, -0.2776, -0.6201, -1.8115,
        -0.8237, -0.2832, -0.7866, -0.7344,  0.3765, -0.9448, -0.9727, -0.5649,
         1.8145,  0.0542, -1.5117, -0.2747, -0.9399,  0.7075,  0.7559, -0.5454,
        -1.1045,  1.9092,  1.1494,  2.5156,  0.0151, -1.8936, -0.1420, -0.1428,
        -0.5386, -1.2881,  0.1127, -0.5259], device='cuda:0',
       grad_fn=<SelectBackward0>)
[12:33:03.897680] Model = Transformer(
  (tok_embeddings): Embedding(32000, 4096)
  (adapter_query): Embedding(300, 4096)
  (criterion): CrossEntropyLoss()
  (layers): ModuleList(
    (0-31): 32 x TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=4096, out_features=4096, bias=False)
        (wk): Linear(in_features=4096, out_features=4096, bias=False)
        (wv): Linear(in_features=4096, out_features=4096, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=11008, bias=False)
        (w2): Linear(in_features=11008, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=11008, bias=False)
      )
      (attention_norm): RMSNorm()
      (ffn_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=32000, bias=False)
)
[12:33:03.897773] base lr: 9.00e-03
[12:33:03.897797] actual lr: 5.62e-04
[12:33:03.897817] accumulate grad iterations: 1
[12:33:03.897834] effective batch size: 16
[12:33:03.937804] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005625
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005625
    maximize: False
    weight_decay: 0.02
)
[12:33:03.938445] Start training for 5 epochs
[12:33:03.940761] log_dir: ./output_dir
[12:33:05.604072] Loss is nan, stopping training
